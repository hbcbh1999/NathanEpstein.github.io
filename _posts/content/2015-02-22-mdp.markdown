---
layout: post
author: Nathan Epstein
title:  "Reinforcement Learning"
categories: jekyll update, content
tweet: "Markov decision processes for reinforcement learning"
---

## Introduction

There are three major categories within machine learning. The most common categories are supervised learning (building a predictive model from labeled input/output data) and unsupervised learning (looking for structure in the data). The third is reinforcement learning. This consists of determining a strategy to optimize a cumulative reward in a multi-step decision process.

## Markov Decision Process

Reinforcement learning problems are generally formalized as Markov Decision Processes (MDP).

Informally, we assume that there are a set of states that we move between probabilistically. In each state, we make actions which affect the likelihood of transitioning to each of the various states in the next step (note: we can also model deterministic systems by assigning transition probabilities of 1). As we move to each new state, we get a reward. Optionally, we can apply a discount to the rewards to account for the fact that payoffs now are preferable to payoffs later. Our goal is to formulate a strategy which maximizes our rewards.

Formally, an MDP is a 5-tuple <span class='eqtn' id='tuple'></span>:

<script type="text/javascript">
  var tuple = '(S,A,P_{sa}, \\gamma ,R)';
  var html = $.parseHTML(katex.renderToString(tuple));
  $('#tuple').append(html);
</script>

<ul>
  <li>
    S is the set of possible states.
  </li>
  <li>
    A is the set of possible actions.
  </li>
  <li>
    <span class='p'></span> specifies the state transition probabilities. For each <span class='s'></span> and each <span class='a'></span>, <span class='p'></span> is a distribution over the state space. Specifically, <span class='p'></span>[s'] is the probability of transitioning from state to s to state s' given action a.
  </li>
  <li>
    <span class='gamma'></span>, the discount factor, is a number in the half-closed interval (0,1]. It specifies how heavily future rewards are discounted.
  </li>
  <li>
    R is the set of rewards for entering each state <span class='s'></span>.
  </li>
</ul>

## Getting a Model

Of course, the real world is unlikely to present us with such a well formulated model. In general, we get data which we can use to estimate the true model. In particular, our objective is to estimate the state-transition probabilities and the rewards (S, A, and <span class='gamma'></span> are likely to be known).

We can observe the transitions to get the maximum likelihood estimates for the state-transition probabilities.

<span class='p'></span>[s'] = (# of times we took action a in state s and went to state s')/(# of times we took action a in state s).

If there are no observations, it is customary to estimate the transition probabilities as being uniform.

Similarly, we can easily estimate R(s) as being the average reward observed in state s.

## Getting a Solution

Once we have estimated a model from the data, we can use that model to determine the optimal policy. To be specific, a policy is a function which maps the state space to actions (i.e. P(s) = a means that we take action a in state s).

We begin by noting that the lifetime value of being in a state is the immediate reward from that state plus the discounted lifetime value of the states we could transition to (multiplied by the probabilities that we transition to those states given our selected action). This result is expressed formally as Bellman's equation.

<span class='bell'></span>

We can use this result to iteratively solve for the optimal policy with a technique called **value iteration**.

1) For each state s, set V(s) = 0

2) While (V(s) not converged){
  <ol>
    <li>
      Update V(s) for each state, <span class='bell'></span>
    </li>
    <li>
      Update the policy, P(s') = a', where a' is the set of actions to get the maximum V above
    </li>
  </ol>
}

Upon convergence, P(s) is the optimal action to take in state s given the estimated model.

##Additional Resources

1) If you'd like more information on reinforcement learning check out <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">these notes.</a>

2) If you'd like a ready for use implementation of reinforcement learning, check out <a href="https://github.com/nathanepstein/reinforce">this library.</a>

<script type="text/javascript">
  var p = $.parseHTML(katex.renderToString('P_{sa}'));
  $('.p').append(p);

  var gamma = $.parseHTML(katex.renderToString('\\gamma'));
  $('.gamma').append(gamma);

  var s = $.parseHTML(katex.renderToString('s \\epsilon S'));
  $('.s').append(s);

  var a = $.parseHTML(katex.renderToString('a \\epsilon A'));
  $('.a').append(a);

  var bell = $.parseHTML(katex.renderToString("V(s) = R(s) + max_{a \\epsilon A} (\\gamma \\sum_{s' \\epsilon S} P_{sa}(s')V(s'))"));
  $('.bell').append(bell);
</script>
